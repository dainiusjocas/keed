\section{SUSIJUSIŲ DARBŲ APŽVALGA}
\label{darbu_apzvalga}

Šiame skyriuje aprašysiu teoriją reikalingą bakalauriniame darbe atliekamam tyrimui: mašininis mokymasis, SVM klasifikatorius, random forest, dimensijų atrinkimo stabilumas.

\subsection{Mašininis mokymasis}

Pataisyti esamą tekstą ir įdėti į bakalaurinį.

\subsubsection{Hierarchinis klasterizavimas}

Šio skyrelio reikia, nes COnsensus Group Stable dimensijų atrinkimo metodas naudoja hierarchinio klasterizavimo algoritmą

\subsection{Atraminių vektorių klasifikatorius}

Pradžia yra kitame faile.

\subsection{\textit{Random Forest} klasifikatorius}

TODO.

\subsection{Dimensijų atrinkimo stabilumas}

Dimensijų atrinkimo metodų stabilumas gali būti apibrėžtas kaip dimensijų atrinkimo rezultatų variacijos dėl mažų pakeitimo duomenų rinkinyje. Pekeitimai duomenų rinkinyje gali būti duomenų objektų lygio (pvz. pridedami ar atimami duomenų objektai), dimensijų lygio (pvz. pridedant dimensijoms triukšmo) ar abiejų lygių kombinacija.

Dimensijų atrinkimo technikų stabilumas yra vis daugiau dėmesio gaunanti tyrimų kryptis. Stabilumo aktualumas yra sąlygotas to, kad biologiniuose duomenyse galima daryti prielaidą, kad konkrečiai problemai yra aktualios tik tam tikros dimensijos. Todėl dalykinės srities ekspertams yra aktualu naudoti tuos dimensijų atrinkimo metodus, kurie yra stabilūs ir relevantiški modeliuojamai problemai, nes tai atpigina tolimesnę duomenų analizę. 

Svarbu paminėti, kad dimensijų stabilumas nėra matuojamas visiškai nepriklausomai - jis yra matuojamas atsižvelgiant į klasifikavimo rezultatus. Matuoti stabilumą verta tada, kai atrinktos dimensijos duoda gerus klasifikavimo rezultatus. Kitaip tariant, nėra naudingi tie dimensijų atrinkimo metodai, kurie duoda labai stabilius rezultatus, bet jais remiantis atrinktomis dimensijomis pavyksta sukurti tik atsitiktinius rezultatus duodančius klasifikatorius.

\subsubsection{Stabilumo matavimas}

Vertinant dimensijų atrinkimo metodų stabilumą yra svarbu kaip panašiai yra atrenkamos dimensijos, kai yra atliekamas dimensijų atrinkimas su vis kitu mėginių ar dimensijų poaibiu. Kuo mažiau skiriasi atrinktoji dimensijų aibė darant pakeitimus duomenyse, tuo dimensijų atrinkimo stabilumas yra didesnis. Vidutinis dimensijų atrinkimo stabilumas gali būti apibrėžtas kaip vidurkis visų reitingavimo metu gautų sąrašų porų tarpusavio panašumo įverčių \cite{kalousis2007stability}:
\begin{equation}
 S_{tot}=\frac{2\sum_{i=1}^{k-1}\sum_{j=i+1}^{k} S(f_i, f_j)}{k*(k-1)},
\end{equation} 
kur $k$ žymi kiek kartų buvo imtas skirtingas poaibis objektų dimensijų atrinkimui,
$f_i$, $f_j$ - dimensijų atrinkimo rezultatas - reitingai, 
$S(f_i, f_j)$ - yra aibių panašumo matavimo funkcija.

Dimensijų atrinkimo stabilumo įvertis priklauso nuo to, kokią aibių panašumo funkciją naudosime. Tradicinės panašumo funkcijos (persidengimo procentas, Pearson'o koreliacija, Spearman'o koreliacijoa) gali būti taikomos, bet jos yra linkusios priskirti didesnes panašumo reikšmes, kai pasirenkamas didesnis dimensijų poaibis. Taip yra dėl padidėjusio sisteminio nuokrypio (ang. bias), nes imant didesnį poaibį padidėja tikimybė tiesiog atsitiktinai pasirinkti dimensiją.

\subsubsection{\textit{Kuncheva} indexas}

\textit{Kuncheva} indexas \cite{DBLP:conf/aia/Kuncheva07} yra funkcija skirta matuoti aibių panašumu. Ši funkcija gerai tinka matuoti dimensijų atrinkimo atabilumą, nes atsižvelgia į paimto dimensijų poaibio dydį. \textit{Kuncheva} indeksas:
\begin{equation}
\label{kuncheva_index}
 KI(f_i, f_j)=\frac{r*N - s^2}{s*(N-s)}=\frac{r - (s^2/N)}{s - (s^2/N)},
\end{equation}		
kur $s=|f_i|=|f_j|$ yra atrinktų dimensijų aibės dydis, $r=|f_i \bigcap f_j|$ - abiems atrinktiems dimensijų poaibiams bendrų dimensijų skaičius, $N$ - bendras  duomenų aibės dimensijų skaičius. Pastebėtina, kad formulėje esantis atėminys $s^2/N$ ištaiso sisteminį nuokrypį atsirandantį dėl galimybės atsitiktinai pasirinkti dimensijas. 

Kunchevos indeksas gali įgyti reikšmes iš intervalo $[-1, 1]$, kur didesnė reikšmė reiškia didesnį panašumą, o artimos nuliui reikšmės reiškia, kad dimensijos atrenkamos daugiausia atsitiktinai. Kunchevos indekso ypatybė yra ta, kad jis atsižvelgia tik į persidengiančias, tačiau visiškai nekreipia dėmesio į koreliuojančias dimensijas.

\subsubsection{\textit{Jaccard} indeksas}

Vienas paprasčiausi aibių panašumo įverčių yra \textit{Jaccard} indeksas \cite{jaccard1901etude}. \textit{Jaccard} indexas yra santykis tarp aibių sankirtos ir aibių sąjungos:
\begin{equation}
\label{jaccard_index}
 JI(f_i, f_j)=\frac{|f_i \bigcap f_j|}{|f_i \cup f_j|}=\frac{\sum_{l}I(f_i^l=f_j^l=1)}{\sum_{l}I{f_i^l+f_j^l > 0)}}, 
\end{equation}
kur $f_i$ ir $f_j$ yra dimensijų reitingai, $I(x)$ - funkcija grąžinanti 1, jei $x=TRUE$, ir 0 kitu atveju.

\subsubsection{\textit{Hamming} atstumas}

Informacijos teorijoje \textit{Hamming} atstumas \cite{hamming1950error} tarp dviejų vienodo ilgio vektorių yra apibrėžtas kaip pozicijų skaičius, kuriose esantys simboliai nesutampa. Kitaip tariant, \textit{Hamming} atstumas yra minimalus skaičius pakeitimų, kad vieną vektorių padarytume lygų kitam. 
\begin{equation}
\label{hamming_distance}
 Hamming Distance(X, Y)= \sum_{i=1}^{n} (x_i \oplus y_i),
\end{equation}
kur $\oplus$ - sumos moduliu 2 arba XOR operacija.

Šiuo metodu matuojant dimensijų atrinkimo stabilumą, prieš atstumo matavimą reikia atlikti pertvarkymus. Pirma, iš atrinktų dimensijų vektorių padaryti bendro dimensijų skaičiaus ilgio binarinius vektorius. Antra, vienetukus sudėti tose vektoriaus elementuose, kurių indeksus gavome dimensijų atrinkimo metodu. Tada jau galima matuoti atstumą tarp dviejų dimensijų atrinkimo rezultatų.
